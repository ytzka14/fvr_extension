{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in ./.conda/lib/python3.12/site-packages (4.0.2)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.51.1-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: tqdm in ./.conda/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./.conda/lib/python3.12/site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in ./.conda/lib/python3.12/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in ./.conda/lib/python3.12/site-packages (from sentence-transformers) (1.15.2)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in ./.conda/lib/python3.12/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in ./.conda/lib/python3.12/site-packages (from sentence-transformers) (4.13.1)\n",
      "Requirement already satisfied: filelock in ./.conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in ./.conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: networkx in ./.conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in ./.conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.conda/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.conda/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.conda/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.conda/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading transformers-4.51.1-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Installing collected packages: huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.17.0\n",
      "    Uninstalling huggingface-hub-0.17.0:\n",
      "      Successfully uninstalled huggingface-hub-0.17.0\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.2\n",
      "    Uninstalling tokenizers-0.15.2:\n",
      "      Successfully uninstalled tokenizers-0.15.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.35.2\n",
      "    Uninstalling transformers-4.35.2:\n",
      "      Successfully uninstalled transformers-4.35.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "summac 0.0.4 requires huggingface-hub<=0.17.0, but you have huggingface-hub 0.30.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface-hub-0.30.2 tokenizers-0.21.1 transformers-4.51.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.conda/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.conda/lib/python3.12/site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.conda/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.conda/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate==0.31.0 in ./.conda/lib/python3.12/site-packages (0.31.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.conda/lib/python3.12/site-packages (from accelerate==0.31.0) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.conda/lib/python3.12/site-packages (from accelerate==0.31.0) (24.2)\n",
      "Requirement already satisfied: psutil in ./.conda/lib/python3.12/site-packages (from accelerate==0.31.0) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in ./.conda/lib/python3.12/site-packages (from accelerate==0.31.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in ./.conda/lib/python3.12/site-packages (from accelerate==0.31.0) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub in ./.conda/lib/python3.12/site-packages (from accelerate==0.31.0) (0.30.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.conda/lib/python3.12/site-packages (from accelerate==0.31.0) (0.5.3)\n",
      "Requirement already satisfied: filelock in ./.conda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.31.0) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.conda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.31.0) (4.13.1)\n",
      "Requirement already satisfied: networkx in ./.conda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.31.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.conda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.31.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.conda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.31.0) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in ./.conda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.31.0) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.conda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.31.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.conda/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate==0.31.0) (1.3.0)\n",
      "Requirement already satisfied: requests in ./.conda/lib/python3.12/site-packages (from huggingface-hub->accelerate==0.31.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.conda/lib/python3.12/site-packages (from huggingface-hub->accelerate==0.31.0) (4.67.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.conda/lib/python3.12/site-packages (from jinja2->torch>=1.10.0->accelerate==0.31.0) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.12/site-packages (from requests->huggingface-hub->accelerate==0.31.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.12/site-packages (from requests->huggingface-hub->accelerate==0.31.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.12/site-packages (from requests->huggingface-hub->accelerate==0.31.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.12/site-packages (from requests->huggingface-hub->accelerate==0.31.0) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.conda/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in ./.conda/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.conda/lib/python3.12/site-packages (from torch) (4.13.1)\n",
      "Requirement already satisfied: networkx in ./.conda/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.conda/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.conda/lib/python3.12/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in ./.conda/lib/python3.12/site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.conda/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.conda/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: summac in ./.conda/lib/python3.12/site-packages (0.0.4)\n",
      "Requirement already satisfied: torch in ./.conda/lib/python3.12/site-packages (from summac) (2.6.0)\n",
      "Requirement already satisfied: transformers>=4.24.0 in ./.conda/lib/python3.12/site-packages (from summac) (4.51.1)\n",
      "Requirement already satisfied: nltk>=3.6.6 in ./.conda/lib/python3.12/site-packages (from summac) (3.9.1)\n",
      "Collecting huggingface-hub<=0.17.0 (from summac)\n",
      "  Using cached huggingface_hub-0.17.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: sentencepiece in ./.conda/lib/python3.12/site-packages (from summac) (0.2.0)\n",
      "Requirement already satisfied: protobuf in ./.conda/lib/python3.12/site-packages (from summac) (6.30.2)\n",
      "Requirement already satisfied: filelock in ./.conda/lib/python3.12/site-packages (from huggingface-hub<=0.17.0->summac) (3.18.0)\n",
      "Requirement already satisfied: fsspec in ./.conda/lib/python3.12/site-packages (from huggingface-hub<=0.17.0->summac) (2025.3.2)\n",
      "Requirement already satisfied: requests in ./.conda/lib/python3.12/site-packages (from huggingface-hub<=0.17.0->summac) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.conda/lib/python3.12/site-packages (from huggingface-hub<=0.17.0->summac) (4.67.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.conda/lib/python3.12/site-packages (from huggingface-hub<=0.17.0->summac) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.conda/lib/python3.12/site-packages (from huggingface-hub<=0.17.0->summac) (4.13.1)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.conda/lib/python3.12/site-packages (from huggingface-hub<=0.17.0->summac) (24.2)\n",
      "Requirement already satisfied: click in ./.conda/lib/python3.12/site-packages (from nltk>=3.6.6->summac) (8.1.8)\n",
      "Requirement already satisfied: joblib in ./.conda/lib/python3.12/site-packages (from nltk>=3.6.6->summac) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.conda/lib/python3.12/site-packages (from nltk>=3.6.6->summac) (2024.11.6)\n",
      "INFO: pip is looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting transformers>=4.24.0 (from summac)\n",
      "  Using cached transformers-4.51.0-py3-none-any.whl.metadata (38 kB)\n",
      "  Using cached transformers-4.50.3-py3-none-any.whl.metadata (39 kB)\n",
      "  Using cached transformers-4.50.2-py3-none-any.whl.metadata (39 kB)\n",
      "  Using cached transformers-4.50.1-py3-none-any.whl.metadata (39 kB)\n",
      "  Using cached transformers-4.50.0-py3-none-any.whl.metadata (39 kB)\n",
      "  Using cached transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
      "INFO: pip is still looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.48.0-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.45.0-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.44.1-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.43.3-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.43.2-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.43.1-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.43.0-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.42.4-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.42.2-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.42.1-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.42.0-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.41.1-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n",
      "  Using cached transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n",
      "  Using cached transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n",
      "  Using cached transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
      "  Using cached transformers-4.39.2-py3-none-any.whl.metadata (134 kB)\n",
      "  Using cached transformers-4.39.1-py3-none-any.whl.metadata (134 kB)\n",
      "  Using cached transformers-4.39.0-py3-none-any.whl.metadata (134 kB)\n",
      "  Using cached transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "  Using cached transformers-4.38.1-py3-none-any.whl.metadata (131 kB)\n",
      "  Using cached transformers-4.38.0-py3-none-any.whl.metadata (131 kB)\n",
      "  Using cached transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
      "  Using cached transformers-4.37.1-py3-none-any.whl.metadata (129 kB)\n",
      "  Using cached transformers-4.37.0-py3-none-any.whl.metadata (129 kB)\n",
      "  Using cached transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "  Using cached transformers-4.36.1-py3-none-any.whl.metadata (126 kB)\n",
      "  Using cached transformers-4.36.0-py3-none-any.whl.metadata (126 kB)\n",
      "  Using cached transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.conda/lib/python3.12/site-packages (from transformers>=4.24.0->summac) (2.2.4)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers>=4.24.0->summac)\n",
      "  Using cached tokenizers-0.15.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.conda/lib/python3.12/site-packages (from transformers>=4.24.0->summac) (0.5.3)\n",
      "Requirement already satisfied: networkx in ./.conda/lib/python3.12/site-packages (from torch->summac) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.conda/lib/python3.12/site-packages (from torch->summac) (3.1.6)\n",
      "Requirement already satisfied: setuptools in ./.conda/lib/python3.12/site-packages (from torch->summac) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.conda/lib/python3.12/site-packages (from torch->summac) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.conda/lib/python3.12/site-packages (from sympy==1.13.1->torch->summac) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.conda/lib/python3.12/site-packages (from jinja2->torch->summac) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.12/site-packages (from requests->huggingface-hub<=0.17.0->summac) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.12/site-packages (from requests->huggingface-hub<=0.17.0->summac) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.12/site-packages (from requests->huggingface-hub<=0.17.0->summac) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.12/site-packages (from requests->huggingface-hub<=0.17.0->summac) (2025.1.31)\n",
      "Using cached huggingface_hub-0.17.0-py3-none-any.whl (294 kB)\n",
      "Using cached transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "Using cached tokenizers-0.15.2-cp312-cp312-macosx_11_0_arm64.whl (2.4 MB)\n",
      "Installing collected packages: huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.30.2\n",
      "    Uninstalling huggingface-hub-0.30.2:\n",
      "      Successfully uninstalled huggingface-hub-0.30.2\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.1\n",
      "    Uninstalling tokenizers-0.21.1:\n",
      "      Successfully uninstalled tokenizers-0.21.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.51.1\n",
      "    Uninstalling transformers-4.51.1:\n",
      "      Successfully uninstalled transformers-4.51.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sentence-transformers 4.0.2 requires huggingface-hub>=0.20.0, but you have huggingface-hub 0.17.0 which is incompatible.\n",
      "sentence-transformers 4.0.2 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.35.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface-hub-0.17.0 tokenizers-0.15.2 transformers-4.35.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./.conda/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in ./.conda/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in ./.conda/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.conda/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in ./.conda/lib/python3.12/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers\n",
    "!pip install pandas\n",
    "!pip install accelerate==0.31.0\n",
    "!pip install torch\n",
    "!pip install summac\n",
    "!pip install nltk\n",
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filepath = './articles_simplified_4.jsonl'\n",
    "articles = pd.read_json(path_or_buf=filepath, lines=True)\n",
    "texts = articles['text']\n",
    "simplifieds = articles['simplified']\n",
    "simplifieds2 = articles['simplified2']\n",
    "simplifieds3 = articles['simplified3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/ytzka14/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ytzka14/github_projects/fvr_extension/others/.conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/ytzka14/github_projects/fvr_extension/others/.conda/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/ytzka14/github_projects/fvr_extension/others/.conda/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/ytzka14/github_projects/fvr_extension/others/.conda/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/ytzka14/github_projects/fvr_extension/others/.conda/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import sklearn.metrics.pairwise as pairwise\n",
    "\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "csv_str = \"article_id, sim1, sim2, sim3\\n\"\n",
    "\n",
    "for i in range(articles.shape[0]):\n",
    "  sentences = [texts[i], simplifieds[i], simplifieds2[i], simplifieds3[i]]\n",
    "  embeddings = model.encode(sentences)\n",
    "  similarities = pairwise.cosine_similarity(embeddings)\n",
    "  sim1 = similarities[0][1]\n",
    "  sim2 = similarities[0][2]\n",
    "  sim3 = similarities[0][3]\n",
    "  csv_str += f\"{i}, {sim1}, {sim2}, {sim3}\\n\"\n",
    "\n",
    "with open('all-MiniLM-L6-v2.csv', 'w') as f:\n",
    "  f.write(csv_str)\n",
    "  f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 1.20k/1.20k [00:00<00:00, 1.61MB/s]\n",
      "Downloading vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 1.34MB/s]\n",
      "Downloading merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 12.5MB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 1.75MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 926kB/s]\n",
      "Downloading config.json: 100%|██████████| 927/927 [00:00<00:00, 2.28MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 1.42G/1.42G [01:54<00:00, 12.5MB/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for dimension 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m probs = logits.softmax(dim=\u001b[32m1\u001b[39m)\n\u001b[32m     14\u001b[39m sim1 = probs[\u001b[32m0\u001b[39m][\u001b[32m1\u001b[39m].item()\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m sim2 = \u001b[43mprobs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m.item()\n\u001b[32m     16\u001b[39m sim3 = probs[\u001b[32m0\u001b[39m][\u001b[32m3\u001b[39m].item()\n\u001b[32m     17\u001b[39m csv_str += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msim1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msim2\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msim3\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mIndexError\u001b[39m: index 2 is out of bounds for dimension 0 with size 2"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"biu-nlp/superpal\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"biu-nlp/superpal\")\n",
    "csv_str = \"article_id, sim1, sim2, sim3\\n\"\n",
    "\n",
    "for i in range(articles.shape[0]):\n",
    "  sentences = [texts[i], simplifieds[i], simplifieds2[i], simplifieds3[i]]\n",
    "  inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "  outputs = model(**inputs)\n",
    "  logits = outputs.logits\n",
    "  probs = logits.softmax(dim=1)\n",
    "  print(probs)\n",
    "  sim1 = probs[0][1].item()\n",
    "  sim2 = probs[0][2].item()\n",
    "  sim3 = probs[0][3].item()\n",
    "  csv_str += f\"{i}, {sim1}, {sim2}, {sim3}\\n\"\n",
    "\n",
    "with open('superpal.csv', 'w') as f:\n",
    "  f.write(csv_str)\n",
    "  f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m conv_csv_str = \u001b[33m\"\u001b[39m\u001b[33marticle_id, sim1, sim2, sim3\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(articles.shape[\u001b[32m0\u001b[39m]):\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m   score_zs = \u001b[43mmodel_zs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43msimplifieds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimplifieds2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimplifieds3\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m   score_conv = model_conv.score([texts[i], texts[i], texts[i]], [simplifieds[i], simplifieds2[i], simplifieds3[i]])\n\u001b[32m     13\u001b[39m   score_zs = score_zs[\u001b[33m\"\u001b[39m\u001b[33mscores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github_projects/fvr_extension/others/.conda/lib/python3.12/site-packages/summac/model_summac.py:382\u001b[39m, in \u001b[36mSummaCZS.score\u001b[39m\u001b[34m(self, sources, generateds, batch_size, **kwargs)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mscore\u001b[39m(\u001b[38;5;28mself\u001b[39m, sources, generateds, batch_size=\u001b[32m128\u001b[39m, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m     images = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerateds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    383\u001b[39m     scores = [\u001b[38;5;28mself\u001b[39m.image2score(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mscores\u001b[39m\u001b[33m\"\u001b[39m: scores, \u001b[33m\"\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m\"\u001b[39m: images}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github_projects/fvr_extension/others/.conda/lib/python3.12/site-packages/summac/model_summac.py:174\u001b[39m, in \u001b[36mSummaCImager.build_images\u001b[39m\u001b[34m(self, originals, generateds, batch_size)\u001b[39m\n\u001b[32m    172\u001b[39m batch_tokens = \u001b[38;5;28mself\u001b[39m.tokenizer.batch_encode_plus(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(batch_prems, batch_hypos)), padding=\u001b[38;5;28;01mTrue\u001b[39;00m, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, max_length=\u001b[38;5;28mself\u001b[39m.max_input_length, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, truncation_strategy=\u001b[33m\"\u001b[39m\u001b[33monly_first\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_tokens\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m batch_probs = torch.nn.functional.softmax(model_outputs[\u001b[33m\"\u001b[39m\u001b[33mlogits\u001b[39m\u001b[33m\"\u001b[39m], dim=-\u001b[32m1\u001b[39m)\n\u001b[32m    177\u001b[39m batch_evids = batch_probs[:, \u001b[38;5;28mself\u001b[39m.entailment_idx].tolist()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github_projects/fvr_extension/others/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github_projects/fvr_extension/others/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github_projects/fvr_extension/others/.conda/lib/python3.12/site-packages/transformers/models/albert/modeling_albert.py:1067\u001b[39m, in \u001b[36mAlbertForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1059\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1060\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m   1061\u001b[39m \u001b[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m   1062\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m   1063\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m   1064\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1065\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1067\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43malbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1068\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1079\u001b[39m pooled_output = outputs[\u001b[32m1\u001b[39m]\n\u001b[32m   1081\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.dropout(pooled_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github_projects/fvr_extension/others/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github_projects/fvr_extension/others/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github_projects/fvr_extension/others/.conda/lib/python3.12/site-packages/transformers/models/albert/modeling_albert.py:732\u001b[39m, in \u001b[36mAlbertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    727\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m    729\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(\n\u001b[32m    730\u001b[39m     input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds\n\u001b[32m    731\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    741\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    743\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler_activation(\u001b[38;5;28mself\u001b[39m.pooler(sequence_output[:, \u001b[32m0\u001b[39m])) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github_projects/fvr_extension/others/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github_projects/fvr_extension/others/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github_projects/fvr_extension/others/.conda/lib/python3.12/site-packages/transformers/models/albert/modeling_albert.py:481\u001b[39m, in \u001b[36mAlbertTransformer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;66;03m# Index of the hidden group\u001b[39;00m\n\u001b[32m    479\u001b[39m group_idx = \u001b[38;5;28mint\u001b[39m(i / (\u001b[38;5;28mself\u001b[39m.config.num_hidden_layers / \u001b[38;5;28mself\u001b[39m.config.num_hidden_groups))\n\u001b[32m--> \u001b[39m\u001b[32m481\u001b[39m layer_group_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43malbert_layer_groups\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgroup_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgroup_idx\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers_per_group\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup_idx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers_per_group\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m hidden_states = layer_group_output[\u001b[32m0\u001b[39m]\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github_projects/fvr_extension/others/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github_projects/fvr_extension/others/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github_projects/fvr_extension/others/.conda/lib/python3.12/site-packages/transformers/models/albert/modeling_albert.py:433\u001b[39m, in \u001b[36mAlbertLayerGroup.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states)\u001b[39m\n\u001b[32m    430\u001b[39m layer_attentions = ()\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer_index, albert_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.albert_layers):\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m     layer_output = \u001b[43malbert_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m     hidden_states = layer_output[\u001b[32m0\u001b[39m]\n\u001b[32m    436\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github_projects/fvr_extension/others/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github_projects/fvr_extension/others/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github_projects/fvr_extension/others/.conda/lib/python3.12/site-packages/transformers/models/albert/modeling_albert.py:396\u001b[39m, in \u001b[36mAlbertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    394\u001b[39m     output_hidden_states: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    395\u001b[39m ) -> Tuple[torch.Tensor, torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m396\u001b[39m     attention_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    398\u001b[39m     ffn_output = apply_chunking_to_forward(\n\u001b[32m    399\u001b[39m         \u001b[38;5;28mself\u001b[39m.ff_chunk,\n\u001b[32m    400\u001b[39m         \u001b[38;5;28mself\u001b[39m.chunk_size_feed_forward,\n\u001b[32m    401\u001b[39m         \u001b[38;5;28mself\u001b[39m.seq_len_dim,\n\u001b[32m    402\u001b[39m         attention_output[\u001b[32m0\u001b[39m],\n\u001b[32m    403\u001b[39m     )\n\u001b[32m    404\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.full_layer_layer_norm(ffn_output + attention_output[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github_projects/fvr_extension/others/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github_projects/fvr_extension/others/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github_projects/fvr_extension/others/.conda/lib/python3.12/site-packages/transformers/models/albert/modeling_albert.py:365\u001b[39m, in \u001b[36mAlbertAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    363\u001b[39m     attention_probs = attention_probs * head_mask\n\u001b[32m--> \u001b[39m\u001b[32m365\u001b[39m context_layer = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    366\u001b[39m context_layer = context_layer.transpose(\u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m).flatten(\u001b[32m2\u001b[39m)\n\u001b[32m    368\u001b[39m projected_context_layer = \u001b[38;5;28mself\u001b[39m.dense(context_layer)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# from summac.model_summac import SummaCZS, SummaCConv\n",
    "\n",
    "# model_zs = SummaCZS(granularity='sentence', model_name='vitc', device='cpu')\n",
    "# model_conv = SummaCConv(models=['vitc'], bins='percentile', granularity='sentence', nli_labels='e', device='cpu', start_file='default', agg='mean')\n",
    "\n",
    "# zs_csv_str = \"article_id, sim1, sim2, sim3\\n\"\n",
    "# conv_csv_str = \"article_id, sim1, sim2, sim3\\n\"\n",
    "\n",
    "# for i in range(articles.shape[0]):\n",
    "#   score_zs = model_zs.score([texts[i], texts[i], texts[i]], [simplifieds[i], simplifieds2[i], simplifieds3[i]])\n",
    "#   score_conv = model_conv.score([texts[i], texts[i], texts[i]], [simplifieds[i], simplifieds2[i], simplifieds3[i]])\n",
    "\n",
    "#   score_zs = score_zs[\"scores\"]\n",
    "#   score_conv = score_conv[\"scores\"]\n",
    "  \n",
    "#   zs_csv_str += f\"{i}, {score_zs[0]}, {score_zs[1]}, {score_zs[2]}\\n\"\n",
    "#   conv_csv_str += f\"{i}, {score_conv[0]}, {score_conv[1]}, {score_conv[2]}\\n\"\n",
    "\n",
    "# with open('SummaCZS.csv', 'w') as f:\n",
    "#   f.write(zs_csv_str)\n",
    "#   f.close()\n",
    "\n",
    "# with open('SummaCConv.csv', 'w') as f:\n",
    "#   f.write(conv_csv_str)\n",
    "#   f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
